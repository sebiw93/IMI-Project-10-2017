---
layout: bloggy
title: 'Is SEO already dead?'
meta: 
source:
category: '#Marketing general'
image: /media/imgs/articles/SEO.jpeg
author: sebastian_weber
permalink: marketing/is-seo-dead
---

<div id="intro">
With every little change in the search engine algorithms the discussion arises, if SEO might be dead. 
As the ranking functionality shift more and more towards a machine learning- based approach, the topic gains a new level of interest. Looking forward, we ask ourselves: What does SEO bring to the year 2018? What can Marketers do in order to keep track with the pace of developments? To keep things short, SEO is not dead, and it won't be in the upcoming years. Nevertheless, it has undergone some major changes in the last years. Let's take a look back on the history of SEO.
</div>

<h1>From keyword stuffing to content optmization</h1>

<h2>Keywords</h2>

Google's search engine started with an algorithm, that focused solely on certain keywords. For the keyword(s) a user is looking for, the engine created an search engine page results page (SERP) by matching the keywords to an index.
Indexes, in the sense of SEO are data stores for all types of content - texts, pictures, videos and others. Whereas owners of a website had to register for those indexes back in the days, Google then started to automate this process. Websites were crawled for usable content, a process that today is updated every 24 hours. To make a website relevant for Google's algorithms, one had to make sure, that the keywords users were looking for appeared on the site in a high quantity. The result was what we know as "keyword stuffing" - an excessive use of keywords, especially in title and description forms of the pages. Later on, also domains were crawled for keywords. Keywords can be of different kind, such as brand- related or generic keywords. Especially for the latter category, Google had problems to rank pages accurately, as they were completely dependent on the keywords, without understanding the meaning behind them. A search for a white toothbrush could bring a completely different SERP as a serach for a red toothbrush, even though the color was not relevant for a buying decision.

<h2>Links and Social media</h2>


Next, Google added links as a factor for its search algorithms. A site that contained links to other websites and that other sites linked onto was identified as a high- quality page, regardless of the perceived quality of those other sites. For this reason, Marketers started building big networks, in which websites just linked back and forth to get high rankings. That was fixed some time later, when Google also measured the "quality" of the linked pages. With the rise of social media, the appearance of one's website could be measured in another form. Not only links were taken into account, but also sharings and links from social platforms. From now on, the content of a certain website was a deciding factor for the search engines: Pages, who where frequently updated and presented appealing content with different (at that time in the Internet quite new) formats as video were ranked a lot higher.

<h2>RankBrain</h2>

Today, Google's algorithms decide autonomously on the ranking of websites based on a huge amount of factors, which are applied to the data in the indexes. All of the above mentioned factors, alongside with technical attributes as the crawling- compability are taken into account. But with its RankBrain, Google made an interesting change in the search engine functionality: Instead of static factors that are analysed and matched between the users keywords and the indexes, Google is now capable of actually understanding more and more of the coherent phrases an individual is searching for. Machine Learning has essentially helped to develop algorithms making this major change possible.

<h1>A new generation of search engines with Machine Learning</h1>

Hola.

---
Google's PageRank
Ein Keyword, auch Suchbegriff, Schlagwort, Schlüsselwort, oder Stichwort, bezeichnet allgemein einen Eingegebenen Begriff in der Suchmaske einer Suchmaschine. Dieses Keyword wird daraufhin von den Suchmaschinen durch ihre Suchalgorithmen behandelt und mit dem jeweiligen Index abgeglichen. Daraufhin bekommt der Nutzer eine Suchergebnisseite (SERP), die Ergebnisse zu diesem Keyword auflistet. Für die Suchmaschinenoptimierung und das Online-Marketing spielen Keywords eine elementare Rolle.

    
- kann mich nicht auf keywords verlassen, content muss der Suchanfrage entsprechen. --> Wechen Dienst bietet die Website an?
Dennoch: Keyword sind essentiell, vor allem im News Sektor (URL, Überschriften & Co), h- Stukturen, auch Description (wird nur dann von Google übernommen, wenn Keywords passen)
-->ABER: Google übernimmt selbst die Optimierung, falls Content gut ist, aber die Keywords schlecht gesetzt sind (Bsp.: Index- Seite hat den Title: Home)
- 2 factors of search engine  result quality:
    - precision (quality)
    - recall (quantity)
    Recall = a/(a+c) --> wichtig bei spezifischen Anfragen --> RankBrain
    Precision = a/(a+b) --> wichtig für generelle Anfragen, wo es eh hunderte Suchergebnisse gibt
    (a – Anzahl der gefundenen relevanten Dokumentationseinheiten, c – Gesamtzahl der relevanten Dokumente, b – Gesamtzahl der gefundenen Dokumente)

    Der Recall oder die Trefferquote ist wie die Precision oder der Fallout ein Maß für die Genauigkeit im Bereich des Information Retrieval. Recall beschreibt den Anteil der relevanten Dokumente aus einer Grundgesamtheit von relevanten Dokumenten, die eine Suchanfrage liefert. Beispiel: Die Grundgesamtheit aller Dokumente betrage 20. Von diesen 20 Elementen sind 10 für eine Suchanfrage relevant. Die Suchanfrage liefert als Ergebnis 5 relevante und 3 nicht relevante Ergebnisse zurück. Der Recall liegt in diesem Fall also bei 5/10=0,5.
    Die Precision oder Genauigkeit ist wie auch der Recall oder der Fallout ein Begriff aus dem Bereich des Information Retrieval. Precision beschreibt den Anteil relevanter Ergebnisse an der Gesamtergebnismenge, die eine Suchanfrage liefert. Beispiel: Eine Suchanfrage liefert 10 relevante und 5 nicht relevante Ergebnisse zurück. Dann beträgt die Precision 10/(10+5)=10/15=0,67
    
-   Personlisierung: nicht eintippen müssen, wo ich grade bin...Google soll selber rausfinden.

-   content entscheidend, dadurch sind auch Suchergebnismöglichkeiten auf Geschäftszweck begrenzt (Suche =                     vorqualifizierender Kanal, nicht nur traffic entscheidend)
    --> Ziel ist nicht, rankings zu optimieren, stattdessen Orientierung am Nutzer!
    
-   wichtiger Faktor: Zeit: Wann will ich warum geranked werden?

--> Problem des Kunden herausfinden: Was sucht er und warum? (Keyworddatenbanken)
Google Search Console: monitor your site

-   nicht nur Relevanz, auch Uniqueness!
- Qualitätskriterium Links ist immer noch relevantm (abhängig von der Qualität der anderen Links), wird allerdings durchmischt mit anderen Faktoren.

-   Merkamle für guten content: Traffic (dafür auch Links!) und longclicks. Bevor Google Websites abwertet müssen erst einmal genügend user signals da sein, daher sind Links v.a. für kleinere Seiten essentiell. Bringen viele Links nicht die angemessene, erwartete traffic, wertet Google die Seite ab.



